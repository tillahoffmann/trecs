# Spotify Recommender

This repository implements a chat-based playlist generator.

## Architecture

The recommender comprises three parts:

1. An orchestrator agent that chats with the user and can use a `recommend(description: str, constraints: ...)` tool once it has gathered enough information to generate recommendations. The description is a textual summary of the requirements the user has specified. This can be any commercially available or open weights model.
2. Once the `recommend` tool is called, a pretrained encoder model maps the textual summary to an embedding vector.
3. A conditional decoder model with cross-attention onto the embedding from step 2. decodes track tokens to generate a new playlist. Any hard `constraints` are applied by modifying the logits of the track predictions, e.g., "only songs from Taylor Swift."

This architecture is motivated by:

* Commerical models are great at chatting, and there is no need to reinvent the wheelâ€”nor could we. We need this chat interface to meet user expectations. The tool call is our way to translate between the conversation and a semantic representation of the user preferences.
* Pretrained encoders already have a good understanding of general cultural context and generate meaningful semantic representations. Using a tool call to generate the description decouples the user interaction from the textual representation that will serve as input to condition a decoder on. This means we can easily swap out different components of the system without re-training. Even swapping out the chat agent or modifying its prompt is acceptable provided the textual input to the encoder has the same semantics.
* The decoder can be pretrained on a large playlist dataset without needing `(description_embedding, playlist)` pairs. We can then fine-tune the cross-attention layer on a smaller dataset with those pairs. This means swapping out the encoder requires re-training the decoder.

### Next steps

* Training the encoder and decoder together as in the seminal "Attention is All You Need" paper.

## Data

- Million playlist dataset from https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge
- Audio feature vectors from:
    1. https://github.com/rezaakb/spotify-recommender
    2. https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks
    3. https://www.kaggle.com/datasets/rodolfofigueroa/spotify-12m-songs
    4. https://www.kaggle.com/datasets/tomigelo/spotify-audio-features
    5. https://github.com/rfordatascience/tidytuesday/tree/main/data/2020/2020-01-21
    6. https://www.kaggle.com/datasets/theoverman/the-spotify-hit-predictor-dataset

All of these data are stuck together into a single sqlite database (see [`schema.sql`](./src/spotify_recommender/schema.sql) for details) which only takes up less than two gigabytes. This database also contains tables to indicate the train-test split so we don't accidentally have leakage.

## Training

### Self-supervised decoder pre-training

We treat playlists as documents and pretrain a GPT-style model on the training set until the validation loss starts increasing. The loss is softmax-cross entropy for the next track.

### Supervised cross-attention training

For a subset of the training set, we generate synthetic textual summaries that serve as the ground truth for training the cross-attention layer. These could be generated by human experts to describe the playlist. But I'm neither an expert, nor do I have the time. So we'll be feeding a text representation of the playlist, including title, song titles, album titles, artist names, and some of the audio features into a commercial model. That model will give us a `description` which is what we'd expect to receive from the tool call of the orchestrator agent. We then train *only* the cross-attention layer by conditioning on the embedding.

### Fine-tuning

Once the cross-attention layers have been trained, we may also want to fine tune the whole thing together. This is an empirical question, because we don't want to overfit the decoder part of the model onto the subset of the training data for which we have generated `description`s.

### Inference (what should be called prediction)

We find the maximum-dot-product items in the database and filter them down based on hard constraints. Once we've identified the top-k items that meet the constraints, we sample from the softmax. This is equivalent to doing top-k sampling while also being efficient usign the nearest neighbor search.

### Next steps

* Include artist and album embeddings, as well as the audio features. This can be achieved by having a dense network to generate the initial context vectors passed to the transformer. We want to make sure that there is an additive component to this network like a resnet because that naturally embeds the tracks, albums, and artists into the same space (or at least linear projections into the same space if they have different embedding dimensions).
